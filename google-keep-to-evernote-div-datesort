
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Google Keep -> Evernote ENEX converter
# - DATES from JSON (createdTimestampUsec / userEditedTimestampUsec) — used for sorting <created>/<updated>
# - Fallback dat: HTML -> mtime (when no JSON)
# - Fixing Unicode special characters: html.unescape + Unicode NFC (title, tags, contents)
# - Divide: max 400 notes in one file
# - Each file is ENEX (header + footer)
# - part01 = oldest notes

import argparse
import os
import sys
import re
import parsedatetime as pdt
import time
import glob
import hashlib
import base64
import json
import html
import unicodedata as ud

cal = pdt.Calendar()

# Regex
r1 = re.compile(r'<li class="listitem checked"><span class="bullet">&#9745;</span>.*?<span class="text">(.*?)</span>.*?</li>')
r2 = re.compile(r'<li class="listitem"><span class="bullet">&#9744;</span>.*?<span class="text">(.*?)</span>.*?</li>')
r3 = re.compile(r'<span class="chip label"><span class="label-name">([^<]*)</span>[^<]*</span>')
r4 = re.compile(r'<img alt="" src="data:(.*?);(.*?)\,(.*?)" />')
r5 = re.compile(r'<div class="content">(.*)</div>')
r6 = re.compile(r'<img alt="" src="(.*?(\.jpg|\.png|\.gif|\.jpeg))" />')

ENEX_HEADER = """<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE en-export SYSTEM "http://xml.evernote.com/pub/evernote-export3.dtd">
<en-export export-date="{export_date}" application="Evernote/Windows" version="6.x">
"""
ENEX_FOOTER = "</en-export>\n"

# ---------- helpers ----------

def readline_until(file, needle):
    currLine = ""
    while needle not in currLine:
        currLine = file.readline()
        if currLine == "":
            break
    return currLine

def read_tags_from_chips(line):
    if line.startswith('<div class="chips">'):
        return line + '\n'
    return ""

def read_images_from_attachment(line, fn):
    dirpath = os.path.dirname(fn)
    attachmentNumber = 0
    result = ()
    m = r4.search(line)
    while m:
        h = hashlib.md5(base64.b64decode(m.group(3).encode("utf-8")))
        newContent = '\n<div><en-media type="{0}" width="1024" hash="{1}" /></div>'.format(m.group(1), h.hexdigest())
        imageFormat = m.group(1).split('/')[1]
        newResource = (
            '<resource><data encoding="{enc}">{data}</data>\n'
            '<mime>{mime}</mime>'
            '<resource-attributes><file-name>IMAGE_FILE_NAME_{idx}.{ext}</file-name></resource-attributes>'
            '</resource>\n'
        ).format(enc=m.group(2), data=m.group(3), mime=m.group(1), idx=attachmentNumber, ext=imageFormat)
        result += (newContent, newResource)
        attachmentNumber += 1
        line = line[m.end():]
        m = r4.search(line)

    m = r6.search(line)
    while m:
        img_path = os.path.join(dirpath, m.group(1))
        with open(img_path, 'rb') as imgfp:
            data = imgfp.read()
        b64 = base64.b64encode(data)
        h = hashlib.md5(data)
        b64utf8 = b64.decode('utf-8')
        extensions = {
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
        }
        _, extension = os.path.splitext(m.group(1).lower())
        img_type = extensions.get(extension, "image/jpeg")
        newContent = '\n<div><en-media type="{t}" width="1024" hash="{h}" /></div>'.format(t=img_type, h=h.hexdigest())
        newResource = (
            '<resource><data encoding="base64">{data}</data>\n'
            '<mime>{mime}</mime>'
            '<resource-attributes><file-name>IMAGE_FILE_NAME_{idx}{ext}</file-name></resource-attributes>'
            '</resource>\n'
        ).format(data=b64utf8, mime=img_type, idx=attachmentNumber, ext=extension)
        result += (newContent, newResource)
        attachmentNumber += 1
        line = line[m.end():]
        m = r6.search(line)

    return result

# ---------- dates & metadata (JSON-first) ----------

def parse_created_from_html(fn):
    """Try to parse human-readable date from HTML; return epoch seconds or None."""
    try:
        with open(fn, 'r', encoding="utf8") as fp:
            readline_until(fp, "<body>")
            fp.readline()    # chips/archived
            fp.readline()    # </div>
            date_line = fp.readline().strip().replace('</div>', '')
        dt, _ = cal.parse(date_line)  # struct_time (local)
        epoch_local = time.mktime(dt)
        return int(epoch_local)
    except Exception:
        return None

def get_timestamps_json_first(fn_html):
    """
    Zwraca (created_epoch, updated_epoch, is_archived) — PRIORYTET JSON.
    Fallback: HTML-created -> mtime; updated=created gdy brak.
    """
    created = updated = None
    is_archived = False
    base, _ = os.path.splitext(fn_html)
    json_path = base + ".json"

    if os.path.exists(json_path):
        try:
            with open(json_path, "r", encoding="utf-8") as jf:
                j = json.load(jf)
            usec_c = j.get("createdTimestampUsec")
            if isinstance(usec_c, str) and usec_c.isdigit():
                usec_c = int(usec_c)
            if isinstance(usec_c, (int, float)):
                created = int(usec_c // 1_000_000)

            usec_u = j.get("userEditedTimestampUsec")
            if isinstance(usec_u, str) and usec_u.isdigit():
                usec_u = int(usec_u)
            if isinstance(usec_u, (int, float)):
                updated = int(usec_u // 1_000_000)

            is_archived = bool(j.get("isArchived", False))
        except Exception:
            pass

    if created is None:
        html_epoch = parse_created_from_html(fn_html)
        if html_epoch is not None:
            created = int(html_epoch)
        else:
            try:
                created = int(os.path.getmtime(fn_html))
            except Exception:
                created = int(time.time())

    if updated is None:
        updated = created

    return int(created), int(updated), is_archived

# ---------- XML escaping (with diacritics fix) ----------

def escape_xml_text(s):
    # Decode HTML entities, normalize Unicode to NFC, then XML-escape
    s = html.unescape(s if s is not None else "")
    s = ud.normalize("NFC", s)
    return (s.replace("&", "&amp;")
             .replace("<", "&lt;")
             .replace(">", "&gt;")
             .replace('"', "&quot;")
             .replace("'", "&apos;"))

# ---------- note builder ----------

def build_note_xml(fn, created_epoch, updated_epoch, is_archived_from_json):
    with open(fn, 'r', encoding="utf8") as fp:
        title = readline_until(fp, "<title>").strip()
        title = title.replace('<title>', '').replace('</title>', '')

        readline_until(fp, "<body>")
        _chips_line = fp.readline()
        resources = ''
        fp.readline()               # </div>
        fp.readline()               # human-readable date (ignored for timestamps)
        fp.readline()               # extra title line

        content = fp.readline()
        m = r5.search(content)
        if m:
            content = m.group(1)
        content = content.replace('<ul class="list">', '')

        for line in fp:
            line = line.strip()
            if line == '</div></body></html>':
                break
            elif line.startswith('<div class="chips">'):
                content += read_tags_from_chips(line)
            elif line.startswith('<div class="attachments">'):
                result = read_images_from_attachment(line, fn)
                i = 0
                while i < len(result):
                    if i + 1 < len(result):
                        content += result[i]
                        resources += result[i+1]
                    i += 2
            else:
                content += line + '\n'

    # treść: unescape + normalizacja (ogonków)
    content = html.unescape(content)
    content = ud.normalize("NFC", content)

    content = content.replace('<br>', '<br/>').replace('\n', '\0')

    while True:
        m = r1.search(content)
        if not m: break
        content = content[:m.start()] + '<en-todo checked="true"/>' + m.group(1) + '<br/>' + content[m.end():]

    while True:
        m = r2.search(content)
        if not m: break
        content = content[:m.start()] + '<en-todo checked="false"/>' + m.group(1) + '<br/>' + content[m.end():]

    content = content.replace('\0', '\n')

    lastUl = content.rfind('</ul>')
    if lastUl != -1:
        content = content[:lastUl] + content[lastUl+5:]

    tags_xml = ''
    m = r3.search(content)
    if m:
        content = content[:m.start()] + content[m.end():]
        tags_xml = '<tag>' + escape_xml_text(m.group(1)) + '</tag>'

    if is_archived_from_json and 'archived' not in tags_xml:
        tags_xml = '<tag>archived</tag>' + (tags_xml or '')

    content = re.sub(r'class="[^"]*"', '', content)

    created_iso = time.strftime('%Y%m%dT%H%M%SZ', time.gmtime(created_epoch))
    updated_iso = time.strftime('%Y%m%dT%H%M%SZ', time.gmtime(updated_epoch))

    note_xml = (
        '  <note>\n'
        '    <title>{title}</title>\n'
        '    <content><![CDATA[<?xml version="1.0" encoding="UTF-8"?>'
        '<!DOCTYPE en-note SYSTEM "http://xml.evernote.com/pub/enml2.dtd">'
        '<en-note style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">{content}</en-note>]]></content>\n'
        '    <created>{created_iso}</created>\n'
        '    <updated>{updated_iso}</updated>\n'
        '    {tags}\n'
        '    <note-attributes>\n'
        '      <latitude>0</latitude>\n'
        '      <longitude>0</longitude>\n'
        '      <source>google-keep</source>\n'
        '      <reminder-order>0</reminder-order>\n'
        '    </note-attributes>\n'
        '    {resources}'
        '  </note>\n'
    ).format(
        title=escape_xml_text(title),
        content=content,
        created_iso=created_iso,
        updated_iso=updated_iso,
        tags=tags_xml,
        resources=resources
    )
    return note_xml

# ---------- sources & chunking (by count) ----------

def expand_sources(sources):
    collected = []
    if not sources:
        collected.extend(glob.glob("*.html"))
    else:
        for item in sources:
            if os.path.isdir(item):
                collected.extend(glob.glob(os.path.join(item, "*.html")))
            else:
                expanded = glob.glob(item)
                if expanded:
                    collected.extend(expanded)
                elif os.path.exists(item):
                    collected.append(item)
    return sorted(set(collected))

def open_chunk(base_path_without_ext, part_index, export_date):
    out_path = f"{base_path_without_ext}_part{part_index:02d}.enex"
    f = open(out_path, "w", encoding="utf8")
    f.write(ENEX_HEADER.format(export_date=export_date))
    return f, out_path

def close_chunk(f):
    f.write(ENEX_FOOTER)
    f.close()

# ---------- main ----------

def main():
    parser = argparse.ArgumentParser(
        description="Convert Google Keep (.html) to Evernote .enex, JSON-dated & chronological, UTF-8 fixed, split by NOTE COUNT."
    )
    parser.add_argument("-o", "--output",
                        help="Base output path; creates <output>_partNN.enex (part01 = oldest). Use sys.stdout for a single file.",
                        default="sys.stdout")
    parser.add_argument("--max-notes-per-file", type=int, default=400,
                        help="Maximum number of notes per output file (default 400).")
    parser.add_argument("htmlSource", nargs="*",
                        help="*.html files/globs or directories from Google Keep Takeout.")
    args = parser.parse_args()

    sources = expand_sources(args.htmlSource)
    if not sources:
        print("No .html file to process", file=sys.stderr)
        sys.exit(1)

    # index with timestamps (JSON-first), sort ascending (oldest first)
    indexed = []
    for fn in sources:
        created_epoch, updated_epoch, is_archived = get_timestamps_json_first(fn)
        indexed.append((created_epoch, updated_epoch, is_archived, fn))
    indexed.sort(key=lambda x: (x[0], x[3]))

    export_date = time.strftime('%Y%m%dT%H%M%SZ', time.gmtime())

    # stdout mode (single file)
    if args.output == "sys.stdout":
        f = sys.stdout
        f.write(ENEX_HEADER.format(export_date=export_date))
        for created_epoch, updated_epoch, is_archived, fn in indexed:
            f.write(build_note_xml(fn, created_epoch, updated_epoch, is_archived))
        f.write(ENEX_FOOTER)
        return

    # split by NOTE COUNT
    max_notes = max(1, int(args.max_notes_per_file))
    base_no_ext, _ = os.path.splitext(args.output)
    part_idx = 1
    note_counter = 0
    current_file, current_path = open_chunk(base_no_ext, part_idx, export_date)

    for created_epoch, updated_epoch, is_archived, fn in indexed:
        if note_counter >= max_notes:
            close_chunk(current_file)
            part_idx += 1
            note_counter = 0
            current_file, current_path = open_chunk(base_no_ext, part_idx, export_date)

        current_file.write(build_note_xml(fn, created_epoch, updated_epoch, is_archived))
        note_counter += 1

    close_chunk(current_file)

if __name__ == "__main__":
    main()
